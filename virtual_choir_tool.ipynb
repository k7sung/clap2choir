{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "virtual choir tool.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u2OdQohbt0Z_"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k7sung/clap2choir/blob/master/virtual_choir_tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzOgv9gHzj8G"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnWiJUwltDGo",
        "cellView": "form"
      },
      "source": [
        "#@title Choir folder location\n",
        "\n",
        "input_dir = '' #@param {type:\"string\"}\n",
        "conductor_video_path = \"Default\" #@param {type:\"string\"}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jRKVrOC3g73F"
      },
      "source": [
        "#@title Parameters for Video Layout\n",
        "VIDEO_RES = \"854x480\" #@param [\"1920x1080\",\"1280x720\", \"854x480\", \"540x360\"] \n",
        "TITLE_H = 0 #@param {type:\"integer\"}\n",
        "CAP_H = 0 #@param {type:\"integer\"} #120\n",
        "MAR_W = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nmi0ApGSgyHj",
        "cellView": "form"
      },
      "source": [
        "#@title Parameters for downscaling\n",
        "RESIZE_TO = 240 #@param {type:\"integer\"}\n",
        "EXTRACT_AUDIO = True #@#param {type:\"boolean\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2OdQohbt0Z_"
      },
      "source": [
        "## codes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmj4wINZ5N-X"
      },
      "source": [
        "#### codes for align"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdqAGuk-PsPu"
      },
      "source": [
        "\n",
        "import os\n",
        "from os.path import join as pathjoin\n",
        "import librosa\n",
        "import numpy as np\n",
        "from librosa.display import waveplot\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn\n",
        "from itertools import combinations as comb \n",
        "import scipy\n",
        "from scipy.ndimage.morphology import grey_closing as closing\n",
        "from os.path import basename, join as pathjoin\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, Dropdown, FloatSlider, Checkbox\n",
        "!pip install ffmpeg-python\n",
        "import ffmpeg\n",
        "import IPython.display\n",
        "\n",
        "def is_conductingvideo(filename):\n",
        "    if filename == conductor_video_path:\n",
        "        return True\n",
        "    lbname = basename(filename).lower()\n",
        "    return ('master' in lbname) or ('conducting' in lbname) or ('conductor' in lbname)\n",
        "\n",
        "def is_right_file(fname):\n",
        "    fname_low = fname.lower()\n",
        "    if \"-handbrake\" in fname_low:\n",
        "        return False\n",
        "    if fname_low.startswith(\"_\"):\n",
        "        return False\n",
        "    for ext in [\".gslides\", \".pdf\", \".jpg\", \".png\", \".gdoc\"]:\n",
        "        if fname_low.endswith(ext):\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def list_files(d):\n",
        "    for fname in os.listdir(d):\n",
        "        fpath = pathjoin(d, fname)\n",
        "        if os.path.isdir(fpath):\n",
        "            continue\n",
        "        if is_right_file(fname):\n",
        "            yield fpath\n",
        "\n",
        "def list_dirs(d):\n",
        "    for fname in os.listdir(d):\n",
        "        fpath = pathjoin(d, fname)\n",
        "        if os.path.isdir(fpath):\n",
        "            yield fpath\n",
        "\n",
        "def get_all_files():\n",
        "    for d in [input_dir]:#list_dirs(input_dir):\n",
        "        yield from list_files(d)\n",
        "    # for fname in list_dirs(input_dir):\n",
        "    #     d = pathjoin(input_dir,fname)\n",
        "    #     if os.path.isdir(d):\n",
        "    #         yield from list_files(d)\n",
        "    #     yield fname\n",
        "\n",
        "def get_files():\n",
        "    return get_all_files()\n",
        "    # for f in get_all_files():\n",
        "    #     if not is_conductingvideo(f):\n",
        "    #         yield f\n",
        "\n",
        "def get_conductor_video():\n",
        "    if conductor_video_path.lower() != \"default\":\n",
        "        return conductor_video_path\n",
        "    for f in get_all_files():\n",
        "        if is_conductingvideo(f):\n",
        "            return f\n",
        "\n",
        "def get_downscale_path(input_dir):\n",
        "    return pathjoin(input_dir, \"edits\")\n",
        "\n",
        "def get_align_data(tracks):\n",
        "    align_to = max([t.get_adjusted_shift_sec() for t in tracks])\n",
        "    to_trim = [(t.path, round(align_to - t.get_adjusted_shift_sec(), 3)) for t in tracks if not t.exclude]\n",
        "    align_data = {}\n",
        "    align_data[\"downscaled_dir\"] = get_downscale_path(input_dir)\n",
        "    align_data[\"file_ss\"] = to_trim\n",
        "    return align_data\n",
        "\n",
        "conductor_video_path = get_conductor_video()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJyRbWepdGxV"
      },
      "source": [
        "\n",
        "SR = 22050\n",
        "N_FFT = 1024\n",
        "STFT_HLEN=4\n",
        "def get_spec(sample):\n",
        "    sample = sample/np.max(sample)\n",
        "    sample_f = librosa.core.stft(sample, n_fft=N_FFT, hop_length=N_FFT//STFT_HLEN)\n",
        "    sample_f = np.abs(sample_f)\n",
        "    _,chan_idxes = (0, list(range(0,256)))\n",
        "#     channels = sample_f[chan_idxes,:]\n",
        "    channels = sample_f[:,:]\n",
        "    channels = channels/np.max(channels)\n",
        "    channels = np.log(channels+0.000000001)\n",
        "#     channels_symbols = np.array([np.mean(channels[:, i:i + SYMBOL_WINDOW], axis=1) for i in range(0, channels.shape[1], SYMBOL_WINDOW//SYMBOL_SAMP)])\n",
        "    return channels\n",
        "# %time y_spec = get_sample_channels(y)\n",
        "# chsums = channels\n",
        "# plt.imshow(y_spec, aspect='auto')\n",
        "\n",
        "\n",
        "\n",
        "def get_loud_regular_patterns(tops, expected_n=4, expected_interval=0):\n",
        "    ts = tops[:,0] #[t for t, v in tops]\n",
        "    ls = tops[:,1] #[v for t, v in tops]\n",
        "    combs = [np.array(c) for c in comb(range(len(ts)), expected_n)]\n",
        "    candidates = []\n",
        "    for cidx in combs:\n",
        "        intervals = np.diff(ts[cidx])\n",
        "        int_mean = np.mean(intervals)\n",
        "        int_deviation = abs(int_mean - expected_interval)\n",
        "        int_var = np.var(intervals)\n",
        "        loudness = np.sum(ls[cidx])\n",
        "        candidates.append([int_deviation, int_var, loudness])\n",
        "    candidates = np.array(candidates)\n",
        "    max_dev, max_var, max_loud = candidates.max(axis=0)\n",
        "    if expected_interval == 0:\n",
        "        w = [0        , 1/max_var, 0.01/max_loud]\n",
        "    else:\n",
        "        w = [1/max_dev, 1/max_var, 0.01/max_loud]\n",
        "    w = np.array(w)\n",
        "    candidate_scores = candidates.dot(w)\n",
        "    n=5\n",
        "    bottom_n_idxs = candidate_scores.argpartition(n)[:n]\n",
        "    # with np.printoptions(precision=3, suppress=True):\n",
        "    # print(\"regulars\", candidates[bottom_n_idxs])\n",
        "    least_of_n_idx = candidate_scores[bottom_n_idxs].argmin()\n",
        "    bottom_idx = bottom_n_idxs[least_of_n_idx]\n",
        "    # print(\"most regular\", candidates[bottom_idx])\n",
        "    # debug_idx=[3,4,5,6]\n",
        "    # for k in candidates:\n",
        "    #     if np.all(k[0] == debug_idx):\n",
        "    #         print(\"val of\", k)\n",
        "    #         break\n",
        "    return ts[combs[bottom_idx]]\n",
        "    # max_var = max([c[0] for c in candi_vars])\n",
        "    # candi_vars = [(c[0] / max_var, c[1]) for c in candi_vars]\n",
        "    # max_loud = max([c[0] for c in candi_louds])\n",
        "    # candi_louds = [(-c[0] / max_loud, c[1]) for c in candi_louds]\n",
        "\n",
        "    #minimize interval deviation,  variance, maximize loudness\n",
        "    # scores = [(1.0 * v[0] + 0.01 * l[0], v[1]) for v, l in zip(candi_vars, candi_louds)]\n",
        "    # # most_regular = min(scores)\n",
        "    # scores.sort()\n",
        "    # print(\"regulars:\", scores[:5])\n",
        "    # most_regular = scores[0]\n",
        "    # print(\"most regular\", most_regular)\n",
        "    # return [ts[t] for t in most_regular[1]]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_sample(fname, duration=None):\n",
        "    print(\"loading \", fname, SR)\n",
        "    sample=None\n",
        "    import warnings\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\")\n",
        "        sample, _ = librosa.load(fname, sr=SR, mono=True, duration=duration) \n",
        "        # sample = sample[:SR*60]\n",
        "    return sample\n",
        "\n",
        "def plot_claps(c, tops):\n",
        "    fname = basename(c.path)\n",
        "    print(\"\\n\" + fname + \"\\n\")\n",
        "    tops = np.array(tops)\n",
        "    # p1 = plt.subplot(211)\n",
        "    # plt.plot(c.t, c.V, '-', color=\"grey\")\n",
        "    # plt.plot(c.t, c.dV, '-', color=\"blue\")\n",
        "    plt.plot(c.t, 0.05*c.V + 1.0*c.dV, '-', color=\"blue\")\n",
        "    plt.plot(tops[:, 0], tops[:, 1], \"x\", color='black')\n",
        "    # p2 = plt.subplot(212)\n",
        "    # plt.plot(peak_ts, d_peaks, '-', color='blue')\n",
        "    plt.plot(c.click_secs, [0] * len(c.click_secs), \"o\", color=\"red\")\n",
        "    # plt.ylim(ymin=-50)\n",
        "    plt.show()\n",
        "\n",
        "def get_window(win_size_ms):\n",
        "    win_sample = librosa.time_to_frames(win_size_ms*0.001, \n",
        "        sr=SR, hop_length=N_FFT//STFT_HLEN)\n",
        "    mid = win_sample//2\n",
        "    # x_dist = [((x-mid)*2/win_sample) for x in range(win_sample)]\n",
        "    # window = [1/(((x-mid)*6/win_sample)**2+0.5) for x in range(win_sample)]\n",
        "    window = 1/(((np.arange(win_sample)-mid)*6/win_sample)**2+0.5)\n",
        "    # plt.plot(x_dist)\n",
        "    # plt.plot(window)\n",
        "    return window\n",
        "\n",
        "window = get_window(1200)\n",
        "# window = get_window(600)\n",
        "\n",
        "def impulses_from_meta(meta):\n",
        "    imp_4 = np.zeros(meta.spec.shape[1])\n",
        "    imp_idx = librosa.time_to_frames(\n",
        "        meta.click_secs, \n",
        "        sr=SR, hop_length=N_FFT//STFT_HLEN\n",
        "    )\n",
        "    imp_4[imp_idx] = 1.0\n",
        "    return imp_4\n",
        "\n",
        "def impulses_from_tops(meta, expected_interval=0.8, top_n=60):\n",
        "    expected_dist = int(expected_interval*SR / (N_FFT/STFT_HLEN)) * 0.5\n",
        "    tops = meta.get_tops(top_n, expected_dist)\n",
        "    imps = np.zeros(meta.spec.shape[1])\n",
        "    idxes = librosa.time_to_frames(tops[:,0], sr=SR, hop_length=N_FFT//STFT_HLEN)\n",
        "    imps[idxes] = tops[:,1]\n",
        "    return imps\n",
        "\n",
        "def get_master_blurred(master_meta):\n",
        "    master_4 = impulses_from_meta(master_meta)\n",
        "    # master_4 = impulses_from_envelope(master_meta.V)\n",
        "    master_4_blurred = np.correlate(master_4, window, 'same')\n",
        "    return master_4_blurred\n",
        "\n",
        "def plot_fine_align(master_meta, signal_meta):\n",
        "    fine_align(master_meta, [signal_meta], make_plot=True)\n",
        "    plt.figure()\n",
        "    # plt.subplot(2,1,1)\n",
        "    plt.title(signal_meta.name)\n",
        "    # plt.plot(t,c)\n",
        "    # plt.plot(librosa.frames_to_time(\n",
        "    #     range(-len(master_4),len(signal_4)-1), hop_length=N_FFT//STFT_HLEN\n",
        "    #     ),\n",
        "    #     corr)\n",
        "    # ax = plt.subplot(2,1,2)\n",
        "    # print(t_shift, maxidx)\n",
        "    ax = plt.gca()\n",
        "    ax.plot(master_meta.t, master_meta.V, color=\"blue\")\n",
        "    ax.plot(signal_meta.t+(master_meta.start_sec-signal_meta.start_sec), signal_meta.V, color=\"red\")\n",
        "    ax.plot(signal_meta.t+signal_meta.shift_sec, signal_meta.V, color=\"green\")\n",
        "    ax.set_xlim(xmax=15)\n",
        "\n",
        "def fine_align(master_meta, signal_meta_arr, make_plot=False): #\n",
        "    master_4 = get_master_blurred(master_meta)\n",
        "    metrics = []\n",
        "    for i, signal_meta in enumerate(signal_meta_arr):\n",
        "        if signal_meta.path == master_meta.path:\n",
        "            continue\n",
        "        # signal_4 = impulses_from_meta(signal_meta)\n",
        "        signal_4 = impulses_from_tops(signal_meta)\n",
        "        corr = np.correlate(master_4, signal_4, \"full\")\n",
        "        maxidx = np.argmax(corr)\n",
        "        score = corr[maxidx]/np.std(corr)\n",
        "        metrics.append([i, corr[maxidx], score])\n",
        "        dt = librosa.frames_to_time(1, hop_length=N_FFT//STFT_HLEN)\n",
        "        t_shift = dt*(maxidx-len(signal_4))\n",
        "        if make_plot:\n",
        "            plt.figure()\n",
        "            plt.plot(librosa.frames_to_time(range(-len(signal_4), len(signal_4)-1), hop_length=N_FFT//STFT_HLEN), corr)\n",
        "            plt.title(f'{signal_meta.name}, {round(corr[maxidx]/np.std(corr),3)}')\n",
        "        if signal_meta.is_master:\n",
        "            signal_meta.shift_sec = 0\n",
        "        else:\n",
        "            signal_meta.shift_sec = round(t_shift, 3)\n",
        "    metrics = np.array(metrics)\n",
        "    plt.figure()\n",
        "    plt.title(\"clap detection\")\n",
        "    plt.plot(metrics[:,1], metrics[:,2], 'o')\n",
        "    plt.xlim(left=0)\n",
        "    plt.ylim(bottom=0)\n",
        "    plt.ylabel(\"consistency\")\n",
        "    plt.xlabel(\"volume\")\n",
        "    for i in metrics:\n",
        "        plt.annotate(int(i[0]), (i[1], i[2]))\n",
        "\n",
        "\n",
        "\n",
        "class VoiceClip:\n",
        "    # FRAME_SIZE = int(SR * 0.01)\n",
        "    # HOP_LEN = FRAME_SIZE//2\n",
        "    \n",
        "    def __init__(self, path):\n",
        "        quiet_threshold = -2400\n",
        "        self.name = basename(path)\n",
        "        self.path = path\n",
        "#         print(VoiceClip.SR)\n",
        "        wav = load_sample(path, duration=30)\n",
        "        self.spec = get_spec(wav)\n",
        "        self.click_secs = []\n",
        "        self.start_sec = 0 #time when clapping ends\n",
        "        self.shift_sec = 0 #time to start the video related to the master. negative means video should be trimmed at the beginning\n",
        "        self.click_intervals = []\n",
        "        self.is_master = is_conductingvideo(path)\n",
        "        self.t = librosa.frames_to_time(range(self.spec.shape[1]), \n",
        "                                   sr=SR, hop_length=N_FFT//STFT_HLEN,\n",
        "                                  )\n",
        "        self.V = np.sum(self.spec[N_FFT//4:,:], axis=0)\n",
        "        self.dV = np.diff(self.V, prepend=0)\n",
        "        self.V = np.max([self.V, [quiet_threshold]*len(self.V)], axis=0)\n",
        "        self.dV = np.diff(self.V, prepend=self.V[0])\n",
        "        self.to_plot = None\n",
        "        self.exclude = False\n",
        "        self.offset = 0.0\n",
        "    \n",
        "    def get_adjusted_shift_sec(self):\n",
        "        if self.exclude:\n",
        "            return 0.0\n",
        "        return self.shift_sec + self.offset\n",
        "\n",
        "#         wav, _ = librosa.load(path, mono=True, sr=VoiceClip.SR, duration=12)\n",
        "#         wav = VoiceClip.normalize(wav)\n",
        "#         wav = librosa.effects.preemphasis(wav)\n",
        "#         wav = librosa.effects.percussive()\n",
        "#         self.ae, self.rms, self.dae = VoiceClip.get_time_features(wav)\n",
        "    def get_tops(self, top_n, expected_dist=50):\n",
        "        # print(\"finding top every\", expected_dist)\n",
        "        # w = lambda v, dv: 0.0 * v + 1.0 * dv\n",
        "        v, dv = self.V, self.dV\n",
        "        wV = 0.05 * v + 1.0 * dv\n",
        "        peak_idxs, _ = scipy.signal.find_peaks(wV, distance=expected_dist)\n",
        "        peak_ts = self.t[peak_idxs]\n",
        "        peak_vals = wV[peak_idxs]\n",
        "\n",
        "        tops = sorted([(t, w) for t, w \n",
        "                    in zip(peak_ts, peak_vals)], \n",
        "                    key=lambda p: p[1], \n",
        "                    reverse=True)[:top_n]\n",
        "        tops = np.array(sorted(tops))\n",
        "\n",
        "        # loudness_all = peak_vals\n",
        "        # loudness_avg = np.mean(loudness_all)\n",
        "        # loudness_std = np.std(loudness_all)\n",
        "        # # loudness_top = np.mean(tops[:,1])\n",
        "        # loudness_top = np.mean(sorted(tops[:,1]))#[len(tops)//2:])\n",
        "        # # loudness_top = np.mean(sorted(tops[:,1])[len(tops)//2:])\n",
        "        # # print(round(loudness_top,2))\n",
        "        # # for i in tops[:,1]:\n",
        "        # #     print(round(i,2),\" \",end=\"\")\n",
        "        # # print()\n",
        "        # # print(\"selected candidate loudness\", round(candidates[bottom_idx, 2],3),\n",
        "        # print(\"(loudness_top - loudness_avg)\", (loudness_top - loudness_avg))\n",
        "        # if (loudness_top - loudness_avg) < 1080:\n",
        "        #     print(\"claps might not be present:\")\n",
        "        #     print(\"selected candidates loudness avg\", round(loudness_top,3),\n",
        "        #         \"\\nloudness avg, std:\", round(loudness_avg,3), round(loudness_std,3),\n",
        "        #         \"\\nperc of std:\", \n",
        "        #         # round((candidates[bottom_idx, 2] - loudness_avg)/loudness_std, 3)*100)\n",
        "        #         round((loudness_top - loudness_avg)/loudness_std*100, 3),\n",
        "        #         \"\\ntop loudness difference\", \n",
        "        #         round((loudness_top - loudness_avg), 3),\n",
        "        #         )\n",
        "        # # print(\"tops: \", tops)\n",
        "        # # plt.plot(peak_ts, peak_vals)\n",
        "        return tops\n",
        "\n",
        "    def estimate_start(self, expected_interval = 0):\n",
        "        # hpass_vol = np.sum(self.spec[N_FFT//4:,:], axis=0)\n",
        "        # self.hapss_vol = hpass_vol\n",
        "        # dVol = np.diff(self.V, prepend=0)\n",
        "        # self.dV = dVol\n",
        "        # peak_idxs, _ = scipy.signal.find_peaks(self.dVol, distance=50)\n",
        "        # peak_ts = self.t[peak_idxs]\n",
        "        # peak_vals = self.dVol[peaks]\n",
        "        if expected_interval == 0:\n",
        "            expected_interval= 0.375 #160bpm, really fast just to be sure\n",
        "            expected_dist = int(expected_interval*SR / (N_FFT/STFT_HLEN))\n",
        "        else:\n",
        "            expected_dist = expected_interval / (self.t[1] - self.t[0]) * 0.8\n",
        "        if self.is_master:\n",
        "            tops = self.get_tops(12, expected_dist=expected_dist)\n",
        "            clap_times = get_loud_regular_patterns(tops, 8, \n",
        "                expected_interval=expected_interval)[-4:]\n",
        "        else:\n",
        "            tops = self.get_tops(12, expected_dist=expected_dist)\n",
        "            clap_times = get_loud_regular_patterns(tops, \n",
        "                expected_interval=expected_interval)\n",
        "        # self.click_secs = self.t[clap_frames]\n",
        "        self.click_secs = clap_times\n",
        "        self.click_intervals = np.diff(self.click_secs)\n",
        "        est_intv = np.mean(self.click_intervals)\n",
        "        self.start_sec = np.mean(self.click_secs) + est_intv * 4.4\n",
        "#         start_est = np.mean(clap_frames) + est_intv * 2\n",
        "#         if is_finetune == False:\n",
        "#             start_est = start_est - est_intv * 4.4\n",
        "        # print(\"start est:\", start_est)\n",
        "        self.to_plot = lambda: plot_claps(self, tops)\n",
        "        return self.start_sec\n",
        "    \n",
        "    def plot_claps(self):\n",
        "        self.to_plot()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyIimVIEg26E"
      },
      "source": [
        "#### widgets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5u0iP5G5NUJ"
      },
      "source": [
        "#prepare alignment widget \n",
        "def update_track_idx(track_widget, offset_widget, exclude_widget, *args): #update calls this first\n",
        "    # track_idx = track_widget.value\n",
        "    track = tracks[track_widget.value]\n",
        "    # offset_widget.value = offsets[track_idx]\n",
        "    offset_widget.value = track.offset\n",
        "    # exclude_widget.value = excludes[track_idx]\n",
        "    exclude_widget.value = track.exclude\n",
        "\n",
        "def update_exclude(track_widget, exclude_widget, *args):\n",
        "    # excludes[track_widget.value] = exclude_widget.value\n",
        "    tracks[track_widget.value].exclude = exclude_widget.value\n",
        "\n",
        "def adjust_offset(track_idx, offset, exclude): #update calls this second, sometimes 3rd\n",
        "    plt.plot(master.t, master.V)\n",
        "    track = tracks[track_idx]\n",
        "    # plt.plot(tracks[track_idx].t + tracks[track_idx].shift_sec + offset, tracks[track_idx].V)\n",
        "    # plt.title(tracks[track_idx].name)\n",
        "    plt.plot(track.t + track.shift_sec + offset, track.V)\n",
        "    plt.title(track.name)\n",
        "    plt.xlim([master.click_secs[0]-3,master.click_secs[-1]+3])\n",
        "    # offsets[track_idx] = offset\n",
        "    track.offset = offset\n",
        "\n",
        "def prepare_manual_align_widget(tracks):\n",
        "    track_widget = Dropdown(options=[(f\"{i} {n}\",i) for i, n in enumerate([t.name for t in tracks])])\n",
        "    offset_widget = FloatSlider(min=-5.0, max=5.0, step=0.05, continuous_update=False)\n",
        "    exclude_widget = Checkbox(False, description=\"do not align\")\n",
        "    track_widget.observe(\n",
        "        lambda *args: update_track_idx(track_widget, offset_widget, exclude_widget, *args), \"value\")\n",
        "    exclude_widget.observe(\n",
        "        lambda *args: update_exclude(track_widget, exclude_widget, *args), \"value\")\n",
        "    interact(adjust_offset, track_idx=track_widget, offset=offset_widget, exclude=exclude_widget);\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdbrJ1JyYgC-"
      },
      "source": [
        "class mix_widget:\n",
        "    def __init__(self, align_data):\n",
        "        self.tracks = get_aligned_clips_path(align_data, [\"audio_only\"])\n",
        "        self.track_widget = None\n",
        "        self.channel_widget = None\n",
        "        self.vol_widgets = []\n",
        "        self.chans_lbl = [\"ch 1\", \"ch 2\", \"ch 3\", \"ch 4\", \"piano\", \"mute\"]\n",
        "        self.chans_vol = [1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n",
        "        self.tracks_to_channels=[0]*len(self.tracks)\n",
        "        self.track_widget = Dropdown(options=\n",
        "                [(f\"{i} {n}\",i) for i, n in enumerate([basename(p) for p in self.tracks])]\n",
        "            )\n",
        "        self.audio_widget = widgets.Audio()\n",
        "        # self.chans = {\"ch 1\":0, \"ch 2\":1, \"ch 3\":2, \"ch 4\":3,\n",
        "        #               \"conductor\":4, \"mute\":5}\n",
        "        self.channel_widget = Dropdown(options=[(v,k) for k,v in enumerate(self.chans_lbl)])\n",
        "        self.track_widget.observe(\n",
        "            lambda *args:self._track_idx_updated(), \"value\")\n",
        "        self.channel_widget.observe(\n",
        "            lambda *args:self._channel_idx_updated(), \"value\")\n",
        "        for lvl, lbl in zip(self.chans_vol, self.chans_lbl):\n",
        "            vol_slider = FloatSlider(value=lvl, min=0, max=1, step=0.05, \n",
        "                description = lbl, orientation=\"vertical\")\n",
        "            if lbl == \"mute\":\n",
        "                vol_slider.disabled = True\n",
        "            vol_slider.observe(lambda *args:self._channel_vol_updated(),\"value\")\n",
        "            self.vol_widgets.append(vol_slider)\n",
        "        \n",
        "    def _track_idx_updated(self):\n",
        "        track_idx = self.track_widget.value\n",
        "        self.channel_widget.value = self.tracks_to_channels[track_idx]\n",
        "    \n",
        "    def _channel_idx_updated(self):\n",
        "        track_idx = self.track_widget.value\n",
        "        self.tracks_to_channels[track_idx] = self.channel_widget.value\n",
        "\n",
        "    def _channel_vol_updated(self):\n",
        "        for i in range(len(self.chans_vol)):\n",
        "            self.chans_vol[i] = self.vol_widgets[i].value\n",
        "\n",
        "    def get_channels_to_tracks(self):\n",
        "        # from collections import defaultdict\n",
        "        # chs = defaultdict(list)\n",
        "        ch2tracks={}\n",
        "        mute_idx = 5\n",
        "        for t, ch in enumerate(self.tracks_to_channels):\n",
        "            if ch == mute_idx:\n",
        "                continue\n",
        "            ch2tracks.setdefault(ch,[]).append(self.tracks[t])\n",
        "        return ch2tracks\n",
        "\n",
        "    def mix(self):\n",
        "        # if not os.path.exists(\"medium_room.wav\"):\n",
        "        #     !curl -LJO \"https://github.com/k7sung/clap2choir/blob/master/medium_room.wav?raw=true\"        \n",
        "        ch2tracks = self.get_channels_to_tracks().items()\n",
        "        # inputs = [ffmpeg.input(t.path) for t in self.tracks]\n",
        "        # channels = [ffmpeg]\n",
        "        channels = []\n",
        "        for ch_idx, ch_tracks in ch2tracks:\n",
        "            vol = self.chans_vol[ch_idx]\n",
        "            inputs = [ffmpeg.input(t_path).audio.filter(\"loudnorm\") for t_path in ch_tracks]\n",
        "            f = (ffmpeg\n",
        "                 .filter(inputs,\"amix\", inputs=len(inputs), duration=\"longest\")\n",
        "                 .filter(\"volume\", vol) )\n",
        "            channels.append(f)\n",
        "        mixed = (ffmpeg.filter(channels, \"amix\", inputs=len(channels), duration=\"longest\")\n",
        "                    .filter(\"aecho\",in_gain=1.0, out_gain=1.0, delays=\"15|25|40\", decays=\"0.5|0.4|0.3\")\n",
        "                   ) #-af aecho=1.0:0.7:20:0.5\n",
        "        # reverb = ffmpeg.input(\"medium_room.wav\")\n",
        "        # out_ch = ffmpeg.filter([mixed, reverb], \"afir\", gtype=\"none\")\n",
        "        out_ch = mixed\n",
        "        return out_ch\n",
        "        \n",
        "    def display(self):\n",
        "        # button = widgets.Button(description=\"preview\")\n",
        "        # def play_audio(*args):\n",
        "        #     print(args)\n",
        "        #     render_audio(m.mix(), \"test.m4a\",ss=10,t=30)\n",
        "        #     self.audio_widget.from_file(\"test.m4a\")\n",
        "        #     # self.audio_widget.reload()\n",
        "        # button.on_click(play_audio)\n",
        "        return widgets.VBox(\n",
        "                [widgets.HBox([self.track_widget, self.channel_widget]),\n",
        "                 widgets.HBox(self.vol_widgets),\n",
        "                #  button,\n",
        "                #  self.audio_widget\n",
        "                 ]\n",
        "            )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uRU9yoV5Te0"
      },
      "source": [
        "#### codes for downscaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2HBIV6b2neA"
      },
      "source": [
        "def has_convert_error(ret_vals):\n",
        "    for t in ret_vals:\n",
        "        if \"Impossible to convert between the formats\" in t:\n",
        "            return True\n",
        "        if \"Unsupported input format:\" in t:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def is_audio(fpath):\n",
        "    fname = basename(fpath).lower()\n",
        "    for ext in [\".wav\", \".mp3\", \".m4a\"]:\n",
        "        if fname.endswith(ext):\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def get_video_height(fpath):\n",
        "    h = !ffprobe -loglevel quiet -select_streams v:0 -show_entries stream=height -of csv=p=0 \"$fpath\"\n",
        "    return int(h[0])\n",
        "\n",
        "def extract_clips(align_data, audio_only=False):\n",
        "    if not audio_only and not os.path.exists(\"colab-ffmpeg-cuda\"):\n",
        "        !git clone https://github.com/rokibulislaam/colab-ffmpeg-cuda.git\n",
        "        !cp -r ./colab-ffmpeg-cuda/bin/. /usr/bin/\n",
        "        !ffmpeg -version\n",
        "\n",
        "    output_dir = align_data[\"downscaled_dir\"]\n",
        "    file_ss = align_data[\"file_ss\"]\n",
        "    !mkdir -p \"$output_dir\"\n",
        "    !mkdir -p \"$output_dir\"/\"audio_only\"\n",
        "    print(len(file_ss), \"files to process\")\n",
        "    i=0\n",
        "    rpixels=RESIZE_TO\n",
        "    bps =\"0.5M\"\n",
        "    hw_cnt, sw_cnt, bypass_cnt = 0,0,0\n",
        "    for f, t in file_ss:\n",
        "        t = round(t,3)\n",
        "        ddir = pathjoin(output_dir, basename(f))\n",
        "        bfname = basename(f)\n",
        "        adir = pathjoin(output_dir, \"audio_only\", f\"{bfname}\")\n",
        "        print(\"file#\",i, bfname, ddir)\n",
        "        i+=1 \n",
        "\n",
        "\n",
        "        if is_audio(f):\n",
        "            print(\"audio only...\")\n",
        "            !ffmpeg -y -ss \"$t\" -i \"$f\" -c:a copy -loglevel error \"$adir\" \n",
        "            # !ffmpeg -y -ss \"$t\" -i \"$f\" -af loudnorm -y -loglevel error \"$adir\" \n",
        "            continue\n",
        "\n",
        "        if EXTRACT_AUDIO:\n",
        "            print(\"extracting audio...\")\n",
        "            !ffmpeg -y -ss \"$t\" -i \"$f\" -c:a copy -vn -loglevel error \"$adir\"\n",
        "\n",
        "        if audio_only:\n",
        "            continue\n",
        "\n",
        "        if \"invisible\" in bfname.lower():\n",
        "            print('video name has \"invisible\", skip...')\n",
        "            bypass_cnt += 1\n",
        "            continue\n",
        "\n",
        "        if os.path.exists(ddir):\n",
        "            print(\"video already processed...\")\n",
        "            bypass_cnt += 1\n",
        "            continue\n",
        "\n",
        "        if get_video_height(f) <= rpixels:\n",
        "            bypass_cnt += 1\n",
        "            print(\"trim video only...\")\n",
        "            !ffmpeg -y -ss \"$t\" -i \"$f\" -c copy -loglevel error \"$ddir\" \n",
        "            continue\n",
        "\n",
        "        hw_cnt+=1\n",
        "        ret_vals = !ffmpeg -y -hwaccel cuda -hwaccel_output_format cuda \\\n",
        "            -ss \"$t\" -i \"$f\" -vf scale_npp=-2:\"$rpixels\" \\\n",
        "            -c:a copy -c:v h264_nvenc \\\n",
        "            -b:v \"$bps\" -vsync 0 \\\n",
        "            -y -loglevel error \"$ddir\"\n",
        "        if has_convert_error(ret_vals):\n",
        "            print(\"plain scale_npp failed, use hwupload\")\n",
        "            ret_vals = !ffmpeg -y -hwaccel cuda -hwaccel_output_format cuda \\\n",
        "            -ss \"$t\" -i \"$f\" -vf \"hwupload_cuda,scale_npp=w=-2:h=\"\"$rpixels\" \\\n",
        "            -c:a copy -c:v h264_nvenc \\\n",
        "            -b:v \"$bps\" -vsync 0 \\\n",
        "            -y -loglevel error \"$ddir\"\n",
        "        if has_convert_error(ret_vals):\n",
        "            hw_cnt-=1\n",
        "            print(\"no luck, use default ffmpeg scaler\")\n",
        "            !ffmpeg -y -ss \"$t\" -i \"$f\" -vf scale=-2:\"$rpixels\" \\\n",
        "            -c:a copy -c:v libx264 -crf 20 -vsync 0 \\\n",
        "            -y -loglevel error \"$ddir\"\n",
        "            # scale_ffmpeg(f, ddir, t, -2, rpixels)\n",
        "            sw_cnt+=1\n",
        "\n",
        "    print(\"GPU processed \",hw_cnt, \"bypassed \", bypass_cnt, \"others\", sw_cnt)   \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2uBwZJg5Wyu"
      },
      "source": [
        "#### codes for compiling final video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8KCI_wD5bh-"
      },
      "source": [
        "W, H = [int(t) for t in VIDEO_RES.split(\"x\")]\n",
        "background_color = (0,0,0)#(255,255,255)\n",
        "H = H - TITLE_H - CAP_H\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "import moviepy\n",
        "# from moviepy.audio.AudioClip import AudioArrayClip\n",
        "from moviepy.editor import VideoFileClip, CompositeVideoClip, clips_array\n",
        "\n",
        "def rotate_vid(vid):\n",
        "    if vid.rotation in [-90, 90, 270]:\n",
        "        print(basename(vid.filename), vid.rotation)\n",
        "        vid = vid.resize(vid.size[::-1])\n",
        "        vid.rotation = 0\n",
        "    return vid\n",
        "\n",
        "def rotate_vids(vids):\n",
        "    for i, v in enumerate(vids):\n",
        "        vids[i] = rotate_vid(v)\n",
        "\n",
        "def scroll_vids(vids, rows):\n",
        "    vids = [v.resize(height=H//rows) for v in vids]\n",
        "    return [vids]\n",
        "\n",
        "def make_srolling_row(vids, rows):\n",
        "    row = clips_array(scroll_vids(vids,rows), bg_color=(0,0,0))\n",
        "    # print(\"before\", row.size)\n",
        "    row = row.crop(y_center=row.h//2, height=H//rows)\n",
        "    # print(\"after\", row.size)\n",
        "    return row\n",
        "\n",
        "def get_scroller(scroll_time):\n",
        "    # print(\"get scroller\")\n",
        "    return lambda get_frame, t: scroll(get_frame, t, scroll_time)\n",
        "\n",
        "def scroll(get_frame, t, scroll_time):\n",
        "    frame = get_frame(t)\n",
        "    # print(frame.size, t)\n",
        "    if frame.shape[1] > W:\n",
        "        scroll_len = frame.shape[1] - W\n",
        "        scroll_speed = scroll_len / scroll_time \n",
        "        scroll_pos = int(t*scroll_speed)\n",
        "        frame_region = frame[:,scroll_pos:scroll_pos+W]\n",
        "    if frame.shape[1] <= W:\n",
        "        frame_region = frame\n",
        "    # if frame.shape[1] < W:\n",
        "    #     scroll_len = W - frame.shape[1]\n",
        "    #     scroll_speed = scroll_len / scroll_time\n",
        "    #     scroll_pos = int(t*scroll_speed)\n",
        "    #     frame_region = \n",
        "    return frame_region\n",
        "\n",
        "# modifiedClip = my_clip.fl( scroll )\n",
        "\n",
        "def make_scrolling_scene(vids, t_start, t_end, *boundary):\n",
        "    clips = [vids[start:end] for start, end in zip(boundary[:-1], boundary[1:])]\n",
        "    arr = [make_srolling_row(clip, len(clips)) for clip in clips]\n",
        "    arr = [row.subclip(t_start,t_end).fl(get_scroller((t_end or row.duration)-t_start)) for row in arr]\n",
        "    # if CAP_H:\n",
        "    #     arr.append([caption_bar.set_duration(arr[0].duration)])\n",
        "    return clips_array([[a] for a in arr], bg_color=background_color).set_start(t_start)#.crossfadein(0.2).crossfadeout(0.2)#.resize(width=W)\n",
        "\n",
        "\n",
        "def crop_vid(vid, exp_W, exp_H):\n",
        "    ar = vid.w/vid.h\n",
        "    expAR = exp_W/exp_H\n",
        "    if ar > expAR:\n",
        "        #wider, need to trim x\n",
        "        vid = vid.resize(height=exp_H-MAR_W)\n",
        "        vid = vid.crop(width = exp_W-MAR_W, x_center=vid.w//2)\n",
        "        # print(vid.size)\n",
        "    else:\n",
        "        #taller, need to trim y\n",
        "        vid = vid.resize(width=exp_W-MAR_W)\n",
        "        vid = vid.crop(height=exp_H-MAR_W, y_center=vid.h//2)\n",
        "    return vid.margin(left=MAR_W, color=background_color)\n",
        "\n",
        "def crop_vids(vids, rows):\n",
        "    # vids = [v.crop(x_center=v.w // 2, width = max(W//len(vids), v.w)) for v in vids]\n",
        "    vids = [crop_vid(v, (W-MAR_W)//len(vids), (H-MAR_W)//rows) for v in vids]\n",
        "    # num,div = W-MAR_W, len(vids)\n",
        "    # widths = [num//div + (1 if x<num % div else 0) for x in range(div)]\n",
        "    # vids = [crop_vid(v, w, (H-MAR_W)//rows) for v,w in zip(vids, widths)]\n",
        "    # print(\"each row\", [v.size for v in vids])\n",
        "    return [vids]\n",
        "\n",
        "def make_row_vids(vids, rows):\n",
        "    # row = clips_array(crop_vids(vids,rows), rows_widths=[W-MAR_W])\n",
        "    row = clips_array(crop_vids(vids,rows))\n",
        "    # print(\"before\", row.size)\n",
        "    # row = row.crop(y_center=row.h//2, height=((H-MAR_W)//rows))\n",
        "    # print(\"after\", row.size)\n",
        "    return row.margin(top=MAR_W, color=background_color)\n",
        "\n",
        "def make_stagger_scene(vids, t_start, t_end, *boundary):\n",
        "    clips = [vids[start:end] for start, end in zip(boundary[:-1], boundary[1:])]\n",
        "    arr = [[make_row_vids(clip, len(clips))] for clip in clips] #each row is only 1 column\n",
        "    arr = [[row[0].subclip(t_start, t_end)] for row in arr]\n",
        "    # if CAP_H:\n",
        "    #     caption_bar = ColorClip((W,CAP_H), color=background_color)\n",
        "    #     arr.append([caption_bar])\n",
        "    arr = clips_array(arr, bg_color=background_color)\n",
        "    arr_w,arr_h = arr.size\n",
        "    # print(max(0, H-arr_h), max(0,W-arr_w))\n",
        "    arr = arr.margin(\n",
        "            right=max(0,W-arr_w), bottom=max(0, H-arr_h)+CAP_H, top=TITLE_H, color=background_color)#.resize(width=W)\n",
        "    return arr.set_start(t_start)#.crossfadein(0.2).crossfadeout(0.2)\n",
        "\n",
        "\n",
        "def dither(n, k): #evenly divide n elements into k groups of integers\n",
        "    r0 = n/k\n",
        "    inner, outer = 0, 0\n",
        "    outs = []\n",
        "    while outer < n:\n",
        "        inner += r0\n",
        "        outer_ = round(inner)\n",
        "        out = outer_ - outer\n",
        "        outs.append(out)\n",
        "        outer = outer_\n",
        "    return outs\n",
        "\n",
        "def stagger(n, ratio=1.5):\n",
        "    import math\n",
        "    k = max(math.floor(math.sqrt(n/ratio)),1)\n",
        "    return dither(n,k)\n",
        "\n",
        "def layout_scene(all_vids):\n",
        "    import itertools\n",
        "    layout = stagger(len(all_vids))\n",
        "    vids_in_rows = [0]+[i for i in layout]\n",
        "    scene = make_stagger_scene(all_vids, 0, None, \n",
        "                                *itertools.accumulate(vids_in_rows))\n",
        "    return scene\n",
        "\n",
        "def render_audio(ff_track, fname, **kvargs):\n",
        "    try:\n",
        "        ff_track.output(fname, **kvargs).overwrite_output().run(capture_stdout=True, capture_stderr=True)\n",
        "    except ffmpeg.Error as e:\n",
        "        print(e.stderr.decode('utf8'))\n",
        "        raise e\n",
        "\n",
        "def get_aligned_clips_path(align_data, parts=[\"\"]):\n",
        "    hymn_dir = align_data[\"downscaled_dir\"]\n",
        "    # parts = [\"\"] \n",
        "    vids={}\n",
        "    for part in parts:\n",
        "        part_dir = pathjoin(hymn_dir, part) \n",
        "        temp = !ls -1N \"$part_dir\"\n",
        "        temp = [(pathjoin(hymn_dir, part, fname), fname) for fname in temp]\n",
        "        vids[part] = [fpath for fpath,fname in temp if os.path.isfile(fpath)]\n",
        "    # for k,v in vids.items():\n",
        "    #     print(k, len(v), \"vids\")\n",
        "    all_vids = []\n",
        "    for p in parts:\n",
        "        all_vids.extend(vids[p])\n",
        "    # print(\"total\", len(all_vids))\n",
        "    return all_vids\n",
        "\n",
        "def preview_video(align_data, arrangement=None):\n",
        "    # OUTPUT_PATH = hymn_dir\n",
        "    all_vids = [VideoFileClip(f) for f in get_aligned_clips_path(align_data)]\n",
        "    if arrangement!=None:\n",
        "        all_vids = [all_vids[i] for i in arrangement]\n",
        "    \n",
        "    rotate_vids(all_vids)\n",
        "    scene = layout_scene(all_vids)\n",
        "    return scene\n",
        "\n",
        "def render_video(align_data, mixer, scene):\n",
        "    output_path = pathjoin(align_data[\"downscaled_dir\"], \"output\")\n",
        "    !mkdir -p \"$output_path\"\n",
        "    print(\"mixing audio...\")\n",
        "    audio_output_path = pathjoin(output_path,\"output.m4a\")\n",
        "    render_audio(mixer.mix(), audio_output_path)\n",
        "    t = CompositeVideoClip([scene])\n",
        "    f_params=['-crf', '21']\n",
        "    output_i = 0\n",
        "    while os.path.exists(pathjoin(output_path, f\"output{output_i}.mp4\")):\n",
        "        output_i += 1\n",
        "    t.write_videofile(pathjoin(output_path, f\"output{output_i}.mp4\"), fps=25, \n",
        "            codec='libx264', \n",
        "            # audio_codec=\"aac\", #audio=pathjoin(OUTPUT_PATH, \"output.m4a\"), \n",
        "            audio=audio_output_path,\n",
        "            ffmpeg_params=f_params, threads=4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5-gJgyGuCMj"
      },
      "source": [
        "## List of the videos to align:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8ic9YKEt90e"
      },
      "source": [
        "if conductor_video_path == None:\n",
        "    print(\"Error!! Conductor video not found.\")\n",
        "else:\n",
        "    print(\"conductor video:\", basename(get_conductor_video()))\n",
        "print(\"files to process:\")\n",
        "[(i, basename(f)) for i,f in enumerate(get_files())]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E88tUr4uYLr"
      },
      "source": [
        "## Load & align the videos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_YL3eIvuUbS"
      },
      "source": [
        "tracks  = [VoiceClip(fname) for fname in get_files()]\n",
        "master= VoiceClip(conductor_video_path)\n",
        "master.estimate_start()\n",
        "fine_align(master, tracks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jo4K3qYex8GD"
      },
      "source": [
        "### manual adjustment if needed..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqv7agWAYA1W"
      },
      "source": [
        "prepare_manual_align_widget(tracks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4IAmA4qfWbq"
      },
      "source": [
        "## Mixer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWq3T9-4-8mK"
      },
      "source": [
        "#every time this cell runs, it resets the mixer adjustments\n",
        "align_data = get_align_data(tracks)\n",
        "extract_clips(align_data, audio_only=True)\n",
        "IPython.display.clear_output()\n",
        "m = mix_widget(align_data)\n",
        "m.display()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_g-R_kj3irL"
      },
      "source": [
        "render_audio(m.mix(), \"preview.m4a\",ss=30,t=15) #ss=start sec of preview; t=duration of preview\n",
        "IPython.display.Audio('preview.m4a', autoplay=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEUhpWsA2928"
      },
      "source": [
        "## Extract the audio and video clips \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBQDLFv729Hd"
      },
      "source": [
        "extract_clips(align_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFmyl_EadJYd"
      },
      "source": [
        "## Generate the choir video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izcSHZNdYfZh"
      },
      "source": [
        "#preview\n",
        "scene = preview_video(align_data)\n",
        "moviepy.editor.ipython_display(scene, t=15, height=360)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSblNi-tdff2"
      },
      "source": [
        "#render\n",
        "render_video(align_data, m, scene)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}